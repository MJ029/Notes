************************************Support Vector Machine Important points************************************
Hint:
	- hard margin is known to be, The SVM allows very low error in classification.
	- The minimum time complexity for training an SVM is O(n2). According to this fact, we can train Large datasets in SVM.
	- Datasets which have a clear classification boundary will function best with SVMâ€™s.
	- The effectiveness of SVM dependes on below.
		- Kernal Trick (Kernal selection)
		- Kernal Parameters
		- Soft Margin Parameter(C)
	- Support vectors are the data points that lie closest to the decision surface.
	- When we are using RBF kernel in SVM with high Gamma value, The model would consider only the points close to the hyperplane for modeling.
	
What happend when C paramter in set to infinite ?
	- When the C parameter is set to infinite, The optimal hyperplane (if exists) will be the one that completely separates the data.
	- At such a high level of misclassification penalty, soft margin will not hold existence as there will be no room for error.

What is Hard-Margin ?
	- A hard margin means that an SVM is very rigid in classification and tries to work extremely well in the training set, causing overfitting.

What is the use of Gama Parameter(decision boundary) in SVM.
	- The gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.
	- If GAMA is low:
		The model will be too constrained and include all points of the training dataset, without really capturing the shape.
	- If GAMA is high:
		The model will capture the shape of the dataset well.
		
