************************************Classification Analysis Important points************************************
Hint:
	- We need to fit n model in n-class classification problem.
	- Random Forest and Gradient Boosting ensemble methods can be used for Regression/Classification analysis.
	- “max_depth” hyperparameter in Gradient Boosting
		- Lower is better parameter in case of same validation accuracy
		- Increase the value of max_depth may underfit the data
	- Both algorithms(Random Forest and Gradient Boosting) can handle real valued attributes by discretizing them
	- Low bias machine learning algorithms - Decision Trees, k-NN and SVM
	- Hight bias machine learning algorithms - Liear Regression, Logistic Regression

How to calculate accuracy, precision, recall using confusion matrix ?
	- Accuracy = (True Positive + True Negative) / Total Observations
	- Precision = (True positive) / (True Positive + False Positive) [or] (Total Actual Possitives)
	- Recall Rate = (True Positive) / (Total Positive + False Negative) [or] (Total predictive Possitives)

Define precision and recall.
	- Recall is also known as the true positive rate: 
		- The amount of positives your model claims compared to the actual number of positives there are throughout the data. 
	- Precision is also known as the positive predictive value:
		- it is a measure of the amount of accurate positives your model claims compared to the number of positives it actually claims.
	- Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. 
	- Of the 8 dogs identified, 5 actually are dogs (true positives), while the rest are cats (false positives).
	- The program's precision is 5/8 while its recall is 5/12.
	
What’s the difference between Type I and Type II error?
	- Type I error is a false positive, while Type II error is a false negative. 
	- Type I error means claiming something has happened when it hasn’t.
	- Type II error means that you claim nothing is happening when in fact something is.
	- In statistical hypothesis testing, a type I error is the rejection of a true null hypothesis (also known as a "false positive" finding),while a type II error is retaining a false null hypothesis (also known as a "false negative" finding)
	- More simply stated, a type I error is to falsely infer the existence of something that is not there, while a type II error is to falsely infer the absence of something that is.
	Note: In statistics, a null hypothesis is a statement that one seeks to nullify with evidence to the contrary.
	
Explain how a ROC curve works. (Receiver operating characteristic)
	- The ROC curve is a graphical representation of the contrast between true positive rates(Recall) and the false positive rate(Precision) at various thresholds.
	- It’s often used as a proxy for the trade-off between 
		- the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).
	- The true-positive rate is also known as sensitivity, recall or probability of detection in machine learning. 
	- The false-positive rate is also known as the fall-out or probability of false alarm and can be calculated as (1 − specificity).
	
Explain the process of building Decision tree ?
- ID3:
	- Find Entropy of target variable
	- Find Entropy of each input attribute
	- Find information Gain of each input attribute
	- Consider the attribute with Highest Information gain(loweest Entropy value) as root element
	- devide dataset to branches
		- If entropy = 0 --> Leaf node
		- If entropy != 0 --> go to step 2 and repeat the same 
- C4.5:
	- find entropy of target variable
	- find information gain and split info of each input attribute
	- find gain raito of each input attribute
	- Consider the attribute with Highest Gain ratio as root element
	- devide dataset to branches
		- If Entropy = 0 --> Leaf Node
		- IF Entropy != 0 --> go to Step - 2 and repeat the same
		
- CART:
	- find Gini Index of each input attribute
	- Calculate Weighted Gini Index  of each input attribute
	- Consider the input attribute with loweest Gini index as root element
	- devide the dataset into sub branches
		- If Gini = 0 --> Leaf node
		- If Gini != 0 --> go to step 2 and repeat the same

You are given a dataset on cancer detection. You have built a classification model and achieved an accuracy of 96 percent. Why shouldn't you be happy with your model performance? What can you do about it?
	- Cancer detection results in imbalanced data. In an imbalanced dataset, accuracy should not be based as a measure of performance.
	- It is important to focus on the remaining four percent, which represents the patients who were wrongly diagnosed.
	- Early diagnosis is crucial when it comes to cancer detection, and can greatly improve a patient’s prognosis.
	- Hence, to evaluate model performance, we should use
		- Sensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine the class wise performance of the classifier.
		
What is Baaging Tree ?
	- In bagging trees, individual trees are independent of each other
	- Bagging is the method for improving the performance by aggregating the results of weak learners
	
What is Boosting tree ?
	- In boosting trees, individual weak learners are independent of each other
	- Its the method for improving the performance by aggregating the results of weak learners

Why is naive Bayes so ‘naive’ ?
	- naive Bayes is so ‘naive’ because it assumes that all of the features in a data set are equally important and independent. 
	- As we know, these assumption are rarely true in real world scenario.
	
How is KNN different from k-means clustering?
	- K-Nearest Neighbors is a supervised classification algorithm
	- in order for K-Nearest Neighbors to work, you need labeled data you want to classify an unlabeled point into (thus the nearest neighbor part). 
	- KNN use the Euclidean distance and a value of k neighbors.
	- It is instance based learning algorithm
	
	- K-means clustering is an unsupervised clustering algorithm. 
	- K-means clustering requires only a set of unlabeled points and a threshold: the algorithm will take unlabeled points and gradually learn how to cluster them into groups by computing the mean of the distance between different points.
	- K-means belongs to the family of moving centroid algorithms
	- It is model based learning algorithm
	
	Note: The critical difference here is that KNN needs labeled points and is thus supervised learning, while k-means doesn’t — and is thus unsupervised learning.
	
What is Bayes’ Theorem? How is it useful in a machine learning context?
	- It finds the probability of an event occuring given the probability of another event occured already. in simple trems it finds the conditional probability of an event occuring given other event.
	
