************************************************************Spark RDD's************************************************
- Driver Program: 
	- At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster.
- The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. 
- RDDs can be created using following 2 ways:
	- referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.
	- parallelizing an existing collection in your driver program.
	Ex:	
		Scala: 
			val data = Array(1, 2, 3, 4, 5)
			val distData = sc.parallelize(data)
		Python:
			data = [1, 2, 3, 4, 5]
			distData = sc.parallelize(data)
- Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations
- RDDs automatically recover from node failures.
- A second abstraction in Spark is shared variables that can be used in parallel operations
- By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task.
- Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. 
- Spark supports two types of shared variables: 
	- broadcast variables: which can be used to cache a value in memory on all nodes
	- accumulators: which are variables that are only “added” to, such as counters and sums.
- Once created, the distributed dataset (distData) can be operated on in parallel. 
	- example, we can call distData.reduce(lambda a, b: a + b) to add up the elements of the list. We describe operations on distributed datasets later on.
*************RDD partition*****************
- A partition (aka split) is a logical chunk of a large distributed data set.
- Ex: 
	Scala:
		val distFile = sc.textFile("data.txt")
	Python:
		distFile = sc.textFile("data.txt")
- Spark manages data using partitions that helps parallelize distributed data processing with minimal network traffic for sending data between executors.
- By default, Spark tries to read data into an RDD from the nodes that are close to it. 
- By default, a partition is created for each HDFS partition, which by default is 64MB (from Spark’s Programming Guide).
- Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks.
- There is a one-to-one correspondence between how data is laid out in data storage like HDFS or Cassandra (it is partitioned for the same reasons).
- number of partitions: is one of the important parameter for parallel collections in RDD which will cut the dataset into parallel partetions.
- Spark will run one task for each partition of the cluster.
- Typically you want 2-4 partitions for each CPU in your cluster.
- Normally, Spark tries to set the number of partitions automatically based on your cluster. 
- However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10))
- In general, smaller/more numerous partitions allow work to be distributed among more workers, 
- but larger/fewer partitions allow work to be done in larger chunks, 
	which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead.
- Increasing partitions count will make each partition to have less data (or not at all!)
- As far as choosing a "good" number of partitions, you generally want at least as many as the number of executors for parallelism. 
- You can get this computed value by calling sc.defaultParallelism.
- Also, the number of partitions determines how many files get generated by actions that save RDDs to files.
- The maximum size of a partition is ultimately limited by the available memory of an executor.
- Partitions get redistributed among nodes whenever shuffle occurs.
- Repartitioning may cause shuffle to occur in some situations, but it is not guaranteed to occur in all cases. And it usually happens during action stage.
- When creating an RDD by reading a file using rdd = SparkContext().textFile("hdfs://…​/file.txt") the number of partitions may be smaller.
- Ideally, you would get the same number of blocks as you see in HDFS, but if the lines in your file are too long (longer than the block size), there will be fewer partitions.
- Preferred way to set up the number of partitions for an RDD is to directly pass it as the second input parameter in the call like 
	rdd = sc.textFile("hdfs://…​/file.txt", 400), where 400 is the number of partitions.

********************************Repartitioning and Coalesce*****************************
- repartition is coalesce with numPartitions and shuffle enabled.
- The coalesce transformation is used to change the number of partitions. 
- It can trigger RDD shuffling depending on the shuffle flag (disabled by default, i.e. false).
- 



Note:
1) How does the number of partitions map to the number of tasks? How to verify it?
2) How does the mapping between partitions and tasks correspond to data locality if any?
3) How Many Partitions Does An RDD Have?
	- Check in Stage UI on spark
	- Check in Storage Ui on spark if ur RDD is cached
	- rdd1.partitions.size will give you the numPartition 
	