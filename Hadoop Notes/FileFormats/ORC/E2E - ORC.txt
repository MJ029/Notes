*********************************************ORC File Format******************************
- the smallest, fastest columnar storage for Hadoop workloads.which supports ACID property.
- ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. 
- It is optimized for large streaming reads, but with integrated support for finding required rows quickly. Storing data in a columnar format lets the reader read, decompress, and process only the values that are required for the current query. 
- Because ORC files are type-aware, the writer chooses the most appropriate encoding for the type and builds an internal index as the file is written.
- Predicate pushdown(PPD) uses those indexes to determine which stripes in a file need to be read for a particular query and the row indexes can narrow the search to a particular set of 10,000 rows.


Advantages of ORC over RC file format:
- a single file as the output of each task, which reduces the NameNode's load
- Hive type support including datetime, decimal, and the complex types (struct, list, map, and union)
- light-weight indexes stored within the file
	- skip row groups that don't pass predicate filtering and seek to a given row
- block-mode compression based on data type
	- run-length encoding for integer columns
	- dictionary encoding for string columns
- metadata stored using Protocol Buffers, which allows addition and removal of fields


File Structure:
- An ORC file contains groups of row data called stripes, along with auxiliary information in a file footer. 
- At the end of the file a postscript holds compression parameters and the size of the compressed footer.
- The default stripe size is 250 MB. Large stripe sizes enable large, efficient reads from HDFS.
- The file footer contains a list of stripes in the file, the number of rows per stripe, and each column's data type.
- It also contains column-level aggregates count, min, max, and sum.
- REFFER: OrcFileLayout.png

Stripe Structure:
- As shown in the diagram, each stripe in an ORC file holds index data, row data, and a stripe footer.
- The stripe footer contains a directory of stream locations.
- Row data is used in table scans.
- Index data includes min and max values for each column and the row positions within each column.
- addition to that A bit field or bloom filter could also be included.
- Note that ORC indexes are used only for the selection of stripes and row groups and not for answering queries.
- Having relatively frequent row index entries enables row-skipping within a stripe for rapid reads, despite large stripe sizes. By default every 10,000 rows can be skipped.

Integer Column Serialization:
- Integer columns are serialized in two streams.
	- present bit stream: is the value non-null?
	- data stream: a stream of integers
- Integer data is serialized in a way that takes advantage of the common distribution of numbers:
- Integers are encoded using a variable-width encoding that has fewer bytes for small integers.
- Repeated values are run-length encoded.
- Values that differ by a constant in the range (-128 to 127) are run-length encoded.
- The variable-width encoding is based on Google's protocol buffers and uses the high bit to represent whether this byte is not the last and the lower 7 bits to encode data. 
- To encode negative numbers, a zigzag encoding is used where 0, -1, 1, -2, and 2 map into 0, 1, 2, 3, 4, and 5 respectively.
- Each set of numbers is encoded this way:
	- If the first byte (b0) is negative:
		- -b0 variable-length integers follow.
	- If the first byte (b0) is positive:
		- it represents b0 + 3 repeated integers
		- the second byte (-128 to +127) is added between each repetition
		- 1 variable-length integer.

String Column Serialization
- Serialization of string columns uses a dictionary to form unique column values. The dictionary is sorted to speed up predicate filtering and improve compression ratios.
- String columns are serialized in four streams.
	present bit stream: is the value non-null?
	dictionary data: the bytes for the strings
	dictionary length: the length of each entry
	row data: the row values

ORC File Dump Utility:

Specifying -d in the command will cause it to dump the ORC file data rather than the metadata (Hive 1.1.0 and later).
Specifying --rowindex with a comma separated list of column ids will cause it to print row indexes for the specified columns, where 0 is the top level struct containing all of the columns and 1 is the first column id (Hive 1.1.0 and later).
Specifying -t in the command will print the timezone id of the writer.
Specifying -j in the command will print the ORC file metadata in JSON format. To pretty print the JSON metadata, add -p to the command.
Specifying --recover in the command will recover a corrupted ORC file generated by Hive streaming.
Specifying --skip-dump along with --recover will perform recovery without dumping metadata.
Specifying --backup-path with a new-path will let the recovery tool move corrupted files to the specified backup path (default: /tmp).
<location-of-orc-file> is the URI of the ORC file.
<location-of-orc-file-or-directory> is the URI of the ORC file or directory. From Hive 1.3.0 onward, this URI can be a directory containing ORC files.



DataTypes:
- its important to notice that ORC files are completely self-describing and do not depend on the Hive Metastore or any other external metadata. 
- The file includes all of the type and encoding information for the objects stored in the file.
- Integer 
	- boolean (1 bit)
	- tinyint (8 bit)
	- smallint (16 bit)
	- int (32 bit)
	- bigint (64 bit)
- Floating point
	- float
	- double
- String types
	- string
	- char
	- varchar
- Binary blobs
	- binary
- Date/time
	- timestamp
	- date
- Complex Types
	- struct
	- list
	- map
	- union
	
- since All ORC file are logically sequences of identically typed objects. All types in ORC can take null values including the compound types.

ACID feature:
- HDFS is a write once file system and ORC is a write-once file format, so edits were implemented using base files and delta files where insert, update, and delete operations are recorded.
- The serialization for the operation codes is
	- INSERT	0
	- UPDATE	1
	- DELETE	2
- in ORC each of the files is sorted by (originalTransaction ascending, bucket ascending, rowId ascending, and currentTransaction descending).
- Each of the files is sorted by (originalTransaction ascending, bucket ascending, rowId ascending, and currentTransaction descending).
- In particular, when a task is reading part of the base file for a bucket, it will use the first and last rowIds to find the corresponding spots in the delta files.
- The hive.acid.key.index lets the reader skip over stripes in the delta file that don’t need to be read in this task.
	- hive.acid.stats		: Number of inserts, updates, and deletes comma separated
	- hive.acid.key.index	: The last originalTransaction, bucket, rowId for each stripe
	
Indexing Feature:
- ORC provides three level of indexes within each file:
	- file level - statistics about the values in each column across the entire file
	- stripe level - statistics about the values in each column for each stripe
	- row level - statistics about the values in each column for each set of 10,000 rows within a stripe
- one of the added advantage of ORC, The file and stripe level column statistics are in the file footer so that they are easy to access to determine if the rest of the file needs to be read at all. 
- Row level indexes include both the column statistics for each row group and the position for seeking to the start of the row group.
- Column statistics always contain the count of values and whether there are null values present.
- Most other primitive types include the minimum and maximum values and for numeric types the sum. 
- As of Hive 1.2, the indexes can include bloom filters, which provide a much more selective filter.
- The indexes at all levels are used by the reader using Search ARGuments or SARGs, which are simplified expressions that restrict the rows that are of interest. 
- For example, if a query was looking for people older than 100 years old, the SARG would be “age > 100” and only files, stripes, or row groups that had people over 100 years old would be read.


*****Table properties*****
- Rerrer ORC config mapping.xlsx


***********Orc with Mapreduce************
https://orc.apache.org/docs/mapred.html

***********Orc API************
https://orc.apache.org/docs/core-java.html


Notes: 
- ORC Doesnot include checksum

*******************************************imp Syntaxs**********************************
1) Create table 
	CREATE TABLE istari (
	  name STRING,
	  color STRING
	) STORED AS ORC;

2) Modifying table:
	ALTER TABLE istari SET FILEFORMAT ORC;
	
3) CONCATENATE
	users can request an efficient merge of small ORC files together on their table or partition. The files will be merged at the stripe level without reserialization.
	ALTER TABLE istari [PARTITION partition_spec] CONCATENATE;

4) orcfiledump
	To get information about an ORC file, use the orcfiledump command.
	hive --orcfiledump <path_to_file>
	
	to display the data in the ORC file
	hive --orcfiledump -d <path_to_file>

5) Table creation with Table properties
	CREATE TABLE istari (
	name STRING,
	color STRING
	) STORED AS ORC TBLPROPERTIES ("orc.compress"="NONE");
	
6) 

*****************************************************************************************

Defaulters:
1) Default Strip size: 64MB, 250 MB.
2) 
